# FILE: modules/career_engine/yok_bot.py (TAM HALÄ°)
import requests
import time
import subprocess
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re

try:
    from googlesearch import search
except ImportError:
    search = None

def find_yok_id_by_name(name):
    print(f"ğŸ•µï¸â€â™‚ï¸ YÃ–K Dedektifi Ä°ÅŸ BaÅŸÄ±nda: '{name}' aranÄ±yor...")
    query = f'site:akademik.yok.gov.tr "{name}"'
    
    # 1. YÃ¶ntem: googlesearch kÃ¼tÃ¼phanesi
    try:
        if search:
            results = search(query, num_results=3, advanced=True)
        else:
            raise ImportError("googlesearch module not found")
        for result in results:
            match = re.search(r'Detay/(\d+)', result.url)
            if match:
                return match.group(1)
    except Exception as e:
        print(f"âš ï¸ Google API HatasÄ±: {e}")

    # 2. YÃ¶ntem: Selenium ile Google Arama (Fallback)
    print("   ğŸŒ Google AramasÄ± (Selenium) deneniyor...")
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-logging")
    options.add_argument("--log-level=3")
    options.add_argument("--silent")
    
    # Pencere flash'Ä±nÄ± engelle
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    options.add_experimental_option("useAutomationExtension", False)
    options.add_argument("--disable-software-rasterizer")
    options.add_argument("--disable-background-timer-throttling")
    options.add_argument("--disable-backgrounding-occluded-windows")
    options.add_argument("--disable-renderer-backgrounding")
    
    driver = None
    try:
        service = Service()
        service.creation_flags = subprocess.CREATE_NO_WINDOW
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(f"https://www.google.com/search?q={query}")
        time.sleep(3)
        
        links = driver.find_elements(By.TAG_NAME, "a")
        for link in links:
            href = link.get_attribute("href")
            if href and "akademik.yok.gov.tr/AkademikArama/Akademisyen/Detay/" in href:
                match = re.search(r'Detay/(\d+)', href)
                if match:
                    return match.group(1)
    except Exception as e:
        print(f"âŒ Selenium Arama HatasÄ±: {e}")
    finally:
        if driver:
            driver.quit()
            
    return None
def scrape_yok_profile(yok_profile_id, name=None):
    """
    YÃ–K profilinden YayÄ±nlar, Projeler, Ã–dÃ¼ller ve Tezleri tek seferde Ã§eker.
    EÄŸer ID hatalÄ±ysa ve 'name' verilmiÅŸse, isme gÃ¶re arayÄ±p doÄŸru profili bulmaya Ã§alÄ±ÅŸÄ±r.
    """
    base_url = "https://akademik.yok.gov.tr/AkademikArama/Akademisyen/Detay/"
    url = f"{base_url}{yok_profile_id}"
    
    # ID uzunsa ve yeni format ise direk URL bu olmayabilir ama deneyelim.
    # EÄŸer ID "12364" gibi kÄ±saysa, muhtemelen hatalÄ±dÄ±r ve isme gÃ¶re arama gerekecektir.
    
    print(f"ğŸŒ YÃ–K Profili TaranÄ±yor (TÃ¼m Veriler): {yok_profile_id}...")

    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-gpu")  # GPU rendering'i kapat (headless iÃ§in Ã¶nemli)
    options.add_argument("--disable-extensions")  # Extension'larÄ± devre dÄ±ÅŸÄ± bÄ±rak
    options.add_argument("--disable-logging")  # Console log'larÄ± kapat
    options.add_argument("--log-level=3")  # Sadece fatal hatalarÄ± gÃ¶ster
    options.add_argument("--silent")  # Sessiz mod
    
    # EKSTRA: Pencere flash'Ä±nÄ± tamamen engelle
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    options.add_experimental_option("useAutomationExtension", False)
    options.add_argument("--disable-software-rasterizer")
    options.add_argument("--disable-background-timer-throttling")
    options.add_argument("--disable-backgrounding-occluded-windows")
    options.add_argument("--disable-renderer-backgrounding")
    
    # SSL/Gizlilik HatalarÄ±nÄ± Yoksay
    options.add_argument('--ignore-certificate-errors')
    options.add_argument('--ignore-ssl-errors')
    options.add_argument('--allow-insecure-localhost')
    options.add_argument('--allow-running-insecure-content')
    
    driver = None
    data = {
        'publications': [],
        'conference_papers': [],  # Separate list for conference papers
        'projects': [],
        'awards': [],
        'theses': [],
        'resolved_id': None # EÄŸer ID deÄŸiÅŸirse buraya yazacaÄŸÄ±z
    }

    try:
        service = Service()
        service.creation_flags = subprocess.CREATE_NO_WINDOW
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)
        time.sleep(3) 

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        page_text = soup.get_text()
        print(f"ğŸ“„ Sayfa BaÅŸlÄ±ÄŸÄ±/Metni (Ä°lk 100): {page_text[:100].strip()}")
        
        # --- HATA KONTROLÃœ VE AKILLI DÃœZELTME ---
        if "Ä°stediÄŸiniz iÅŸleme cevap veremiyoruz" in page_text or "Records not found" in page_text:
            print("âŒ ID ile direkt eriÅŸim baÅŸarÄ±sÄ±z.")
            
            if name:
                # Ä°smi temizle (Unvanlardan arÄ±ndÄ±r)
                raw_name = name
                titles = [r'Prof\.?', r'Dr\.?', r'Doc\.?', r'DoÃ§\.?', r'ArÅŸ\.?', r'GÃ¶r\.?', r'Ã–ÄŸr\.?', r'Ãœyesi\.?', r'Yrd\.?']
                for t in titles:
                    name = re.sub(t, '', name, flags=re.IGNORECASE).strip()
                
                print(f"ğŸ”„ AkÄ±llÄ± Mod: '{raw_name}' (Aranan: '{name}') ismiyle YÃ–K iÃ§inde aranÄ±yor...")
                try:
                    driver.get("https://akademik.yok.gov.tr/AkademikArama/")
                    time.sleep(2)
                    
                    # Arama Kutusunu Bul
                    search_box = driver.find_element(By.ID, "aramaTerim")
                    search_box.clear()
                    search_box.send_keys(name)
                    
                    # Ara Butonu
                    search_btn = driver.find_element(By.ID, "searchButton")
                    search_btn.click()
                    print("â¡ï¸ Arama butonu tÄ±klandÄ±. 3 saniye bekleniyor...")
                    time.sleep(3)
                    
                    # "Akademisyenler" sekmesine geÃ§ (Garanti olsun)
                    try:
                        print("ğŸ‘€ 'Akademisyenler' sekmesi aranÄ±yor...")
                        akademisyen_tab = WebDriverWait(driver, 5).until(
                            EC.element_to_be_clickable((By.XPATH, "//a[contains(text(), 'Akademisyenler')]"))
                        )
                        akademisyen_tab.click()
                        print("â¡ï¸ 'Akademisyenler' sekmesi tÄ±klandÄ±. 2 saniye bekleniyor...")
                        time.sleep(2)
                    except Exception as tab_e:
                        print(f"âš ï¸ 'Akademisyenler' sekmesi bulunamadÄ± veya tÄ±klanamadÄ±: {tab_e}")
                        # Devam et, belki varsayÄ±lan sekmedir.
                        
                    start_time = time.time()
                    found_profile = False
                    
                    while time.time() - start_time < 10: # 10 saniye bekle
                        all_links = driver.find_elements(By.TAG_NAME, "a")
                        for link in all_links:
                             try:
                                 href = link.get_attribute("href")
                                 text = link.text
                                 
                                 if href and ("viewAuthor.jsp" in href or "AkademisyenGorevOgrenimBilgileri" in href):
                                     print(f"   ğŸ” ADAY BULUNDU: '{text}' (Aranan: '{name}') URL: {href}")
                                     
                                     # Ä°sim kontrolÃ¼
                                     search_term = name.split()[0].lower()
                                     link_text_lower = text.lower()
                                     
                                     mapping = {'Ä°': 'i', 'I': 'Ä±', 'Å': 's', 'Ä': 'g', 'Ãœ': 'u', 'Ã–': 'o', 'Ã‡': 'c'}
                                     for k, v in mapping.items():
                                         link_text_lower = link_text_lower.replace(k.lower(), v)
                                         search_term = search_term.replace(k.lower(), v)

                                     if search_term in link_text_lower: 
                                         print(f"âœ… EÅLEÅTÄ°: {text} - URL: {href}")
                                         
                                         # ID Ã‡Ã¶zÃ¼mleme
                                         match_id = re.search(r'authorId=([a-zA-Z0-9]+)', href)
                                         if match_id:
                                             data['resolved_id'] = match_id.group(1)
                                             print(f"ğŸ†” Linkten ID Ã‡Ã¶zÃ¼ldÃ¼: {data['resolved_id']}")

                                         # YÃ–K Session HatasÄ± almamak iÃ§in DOÄRUDAN LÄ°NKE TIKLA
                                         # driver.get() yeni istek yaptÄ±ÄŸÄ± iÃ§in session kopabiliyor.
                                         
                                         print(f"ğŸ”— SonuÃ§ linkine tÄ±klanÄ±yor: {href}")
                                         
                                         # Target blank kaldÄ±r
                                         driver.execute_script("arguments[0].removeAttribute('target')", link)
                                         time.sleep(0.5)
                                         
                                         # TÄ±kla
                                         driver.execute_script("arguments[0].click();", link)
                                         
                                         found_profile = True
                                         
                                         # Yeni sayfanÄ±n yÃ¼klenmesini bekle
                                         time.sleep(5) 
                                         break
                                     else:
                                         print(f"âŒ EÅLEÅMEDÄ°: '{search_term}' vs '{link_text_lower}'")

                             except:
                                 continue
                        if found_profile: break
                        time.sleep(1)

                    if found_profile:
                        # SayfanÄ±n yÃ¼klenmesini bekle
                        time.sleep(3)
                        print("âœ… Profil sayfasÄ± yÃ¼klendi, HTML analizi baÅŸlÄ±yor...")
                        
                        soup = BeautifulSoup(driver.page_source, 'html.parser')
                        
                        # --- YENÄ° SÄ°STEM SCRAPING (viewAuthorArticle.jsp) ---
                        # Bu sayfada makaleler genellikle bir liste veya tablo iÃ§indedir.
                        # KullanÄ±cÄ± screenshot'Ä±na gÃ¶re: <span>Diffraction...</span> gibi.
                        
                        # TÃ¼m metni alÄ±p satÄ±r satÄ±r analiz edelim, daha gÃ¼venli.
                        # Veya belirli class'larÄ± arayalÄ±m. Genellikle 'list-group-item' veya benzeri.
                        
                        # Ã–nce basitÃ§e tÃ¼m satÄ±rlarÄ± (tr) ve divleri tarayalÄ±m.
                        # BaÅŸlÄ±klarÄ± yakalamak iÃ§in en garantisi: YÄ±l iÃ§eren satÄ±rlarÄ± bulmak.
                        
                        rows = soup.find_all(['tr', 'div', 'p', 'span'])
                        pubs = []
                        for row in rows:
                            text = row.get_text().strip()
                            if len(text) > 20 and re.search(r'20[0-2][0-9]', text):
                                # Gereksizleri ele
                                if "Telif" in text or "Listele" in text or "SonuÃ§lar" in text: continue
                                # Temizle (Bazen sÄ±ra no baÅŸta olur "1. Title")
                                clean = re.sub(r'^\d+\.?\s*', '', text)
                                pubs.append(clean)
                        
                        # Duplicate temizle ve kaydet
                        data['publications'] = list(set(pubs))
                        print(f"   -> Makale (Tahmini): {len(data['publications'])} Ã¶ge")

                        # DiÄŸer sekmeler (Proje, Ã–dÃ¼l) iÃ§in URL deÄŸiÅŸimi gerekebilir
                        # Åimdilik ana hedef Makaleler (Publications). 
                        # EÄŸer proje istenirse: viewAuthorProject.jsp vb. tahmin edilebilir ama ÅŸimdilik ID Ã§Ã¶zÃ¼ldÃ¼ÄŸÃ¼ iÃ§in yeterli.
                        
                    else:
                        print("âŒ Ä°simle arama sonucunda kayÄ±t bulunamadÄ±.")
                        return data
                except Exception as e:
                    print(f"âŒ AkÄ±llÄ± Arama HatasÄ±: {e}")
                    return data
            else:
                return data

        # --- 1. YAYINLAR ---
        # Genellikle varsayÄ±lan sekme yayÄ±nlardÄ±r veya "Makale" sekmesi.
        # YÃ–K yapÄ±sÄ±nda sekmeler genellikle id ile ayrÄ±lÄ±r.
        # TÃ¼m tablo satÄ±rlarÄ±nÄ± (tr) alalÄ±m, ama sekmeleri gezmek daha garanti.
        # Åimdilik basitÃ§e gÃ¶rÃ¼neni alalÄ±m, sonra tÄ±klayalÄ±m.
        
        # --- SEKMELERÄ° GEZME MANTIÄI ---
        # YÃ–K SayfasÄ±nda sekmelerin ID'leri genellikle:
        # id="tab_Article" -> Makaleler
        # id="tab_Project" -> Projeler
        # id="tab_Award" -> Ã–dÃ¼ller
        # id="tab_Thesis" -> Tezler
        # (Bu ID'ler tahminidir, ancak genellikle text iÃ§eriÄŸine gÃ¶re tÄ±klayabiliriz)

        tabs_to_click = {
            "Makale": "publications",
            "Bildiri": "conference_papers",  # Conference papers section
            # "Proje": "projects",
            # "Ã–dÃ¼l": "awards",
            # "Tez": "theses" # "YÃ¶netilen Tezler" - User requested only publications
        }

        for tab_text, key in tabs_to_click.items():
            print(f"ğŸ‘‰ Processing Tab: {key} ({tab_text})")
            try:
                # Sekmeyi bul ve tÄ±kla (kÄ±smi eÅŸleÅŸme)
                try:
                     # . kullanmak, iÃ§ iÃ§e elementlerdeki text'i de kapsar (Ã¶r: <span>Makale</span>)
                     tab_element = driver.find_element(By.XPATH, f"//a[contains(., '{tab_text}')]")
                     driver.execute_script("arguments[0].click();", tab_element)
                     time.sleep(2) # YÃ¼klenmesini bekle
                except Exception as click_e:
                     # EÄŸer zaten Makale sayfasÄ±ndaysak ve "Makale" sekmesi tÄ±klanabilir deÄŸilse (active class vs) hata verebilir.
                     # Bu durumda devam etmeliyiz, Ã§Ã¼nkÃ¼ zaten oradayÄ±zdÄ±r.
                     if key == "publications":
                         print(f"   âš ï¸ '{tab_text}' tÄ±klanamadÄ± (Zaten aktif olabilir), devam ediliyor... Hata: {click_e}")
                     else:
                         raise click_e # DiÄŸer sekmeler iÃ§in kritik hata
                
                # --- INFINITE SCROLL ---
                # YÃ–K Lazy Load kullanÄ±yor, tÃ¼m veriyi Ã§ekmek iÃ§in aÅŸaÄŸÄ± kadar inmeliyiz.
                
                # 1. Strateji: Window Scroll
                last_height = driver.execute_script("return document.body.scrollHeight")
                stuck_counter = 0
                
                for i in range(10): # 15 -> 10'a dÃ¼ÅŸÃ¼rÃ¼ldÃ¼, yeterli.
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(1) # 2s -> 1s'e dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ hÄ±z iÃ§in.
                    
                    # 2. Strateji: HTML/Body Scroll (BazÄ± siteler iÃ§in)
                    driver.execute_script("document.documentElement.scrollTop = document.documentElement.scrollHeight")
                    
                    new_height = driver.execute_script("return document.body.scrollHeight")
                    
                    if new_height == last_height:
                        stuck_counter += 1
                        if stuck_counter >= 2: # 2 kere deÄŸiÅŸmezse dur
                            break
                    else:
                        stuck_counter = 0
                    last_height = new_height

                # Ä°Ã§eriÄŸi parse et
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # --- AKILLI ELEMENT SEÃ‡Ä°CÄ° ---
                # Hem tablo satÄ±rlarÄ±nÄ± (tr) hem de modern gÃ¶rÃ¼nÃ¼mdeki (div.list-group-item vb.) Ã¶ÄŸeleri ara
                candidate_elements = soup.find_all(['tr', 'div', 'p', 'li'])
                
                fetched_items = []
                seen_normalized = set()
                
                print(f"   ğŸ“Š Analiz edilen Ã¶ÄŸe sayÄ±sÄ±: {len(candidate_elements)}")

                for el in candidate_elements:
                    # SatÄ±rÄ±n ham metni
                    full_text = el.get_text(" ", strip=True) # BoÅŸlukla birleÅŸtir
                    if not full_text or len(full_text) < 10: continue
                    
                    # YÄ±l KontrolÃ¼ (En belirleyici Ã¶zellik)
                    # 1990-2030 arasÄ± yÄ±llarÄ± ara
                    year_matches = re.findall(r'\b(199[0-9]|20[0-2][0-9])\b', full_text)
                    if not year_matches:
                        continue # YÄ±l yoksa makale deÄŸildir (genellikle)
                    
                    found_year = year_matches[-1] # Genellikle en sondaki yÄ±l yayÄ±n yÄ±lÄ±dÄ±r
                    
                    # BaÅŸlÄ±k AyÄ±klama
                    # Genellikle yÄ±lÄ±n Ã¶ncesindeki uzun metin parÃ§asÄ± baÅŸlÄ±ktÄ±r.
                    # Ancak tam baÅŸlÄ±ÄŸÄ± Ã§ekmek zor olabilir, bu yÃ¼zden temizlenmiÅŸ full text'i alalÄ±m
                    # veya eleman iÃ§indeki <a> tag'ine bakalÄ±m (Ã§oÄŸu makale linklidir)
                    
                    title_candidate = ""
                    link_text = ""
                    
                    # Varsa linkin textini al (En temizi)
                    a_tag = el.find('a')
                    if a_tag:
                         link_text = a_tag.get_text(strip=True)
                    
                    if link_text and len(link_text) > 20:
                        title_candidate = link_text
                    else:
                        # Link yoksa metni temizle
                        # Tarihleri ve "Telif", "Yazar" gibi keywordleri at
                        clean_text = full_text
                        for y in year_matches:
                            clean_text = clean_text.replace(y, "")
                        
                        # Kalan en uzun parÃ§a muhtemelen baÅŸlÄ±ktÄ±r
                        parts = [p.strip() for p in clean_text.split() if len(p.strip()) > 3]
                        if len(parts) > 3:
                            title_candidate = " ".join(parts) # Basit birleÅŸtirme
                        else:
                            title_candidate = full_text # Fallback
                            
                    # --- NOISE FILTER ---
                    # UI elementlerini temizle
                    noise_keywords = ["Toggle navigation", "YÃ¼ksekÃ¶ÄŸretim Kurulu", "KiÅŸisel Bilgiler", 
                                      "Telif HakkÄ±", "English", "Anasayfa", "Birlikte Ã§alÄ±ÅŸtÄ±ÄŸÄ± kiÅŸiler", 
                                      "DetaylÄ± Arama", "Akademisyenler", "BÃ¶lÃ¼mler"]
                    if any(nk in title_candidate for nk in noise_keywords):
                        continue

                    # --- URL/DOI FILTER (NEW) ---
                    # Check if the title candidate looks like a URL
                    tc_lower = title_candidate.lower()
                    if "http" in tc_lower or "www." in tc_lower or "doi.org" in tc_lower or "dx.doi" in tc_lower:
                        continue
                    
                    # Check if it's just a single long word (likely a compacted URL)
                    if " " not in title_candidate.strip() and len(title_candidate) > 20:
                        continue

                    # Son dÃ¼zenleme
                    final_title = title_candidate
                    if found_year not in final_title:
                         final_title = f"{final_title} ({found_year})"

                    # Filtreleme
                    if key == "publications":
                        # Makale sekmesindeyiz, yÄ±l bulduk, yeterli.
                        pass
                    
                    elif key == "conference_papers":
                        # Bildiri sekmesindeyiz, yÄ±l bulduk, yeterli.
                        pass
                    
                    elif key == "projects" and ("Proje" in full_text or "TÃœBÄ°TAK" in full_text or "BAP" in full_text):
                        pass
                         
                    elif key == "awards" and ("Ã–dÃ¼l" in full_text or "Award" in full_text):
                        pass
                    
                    elif key == "theses" and ("Tez" in full_text or "DanÄ±ÅŸman" in full_text):
                        pass
                    else:
                        # EÄŸer sekme spesifik keyword yoksa ama genel yapÄ± uyuyorsa (YÄ±l var, uzun metin var)
                        # ve biz 'publications' veya 'conference_papers' sekmesindeysek kabul et.
                        if key not in ["publications", "conference_papers"]:
                            continue

                    # --- TYPE DETECTION ---
                    pub_type = 'Other'
                    full_text_lower = full_text.lower()
                    if 'makale' in full_text_lower or 'article' in full_text_lower:
                        pub_type = 'Journal'
                    elif 'bildiri' in full_text_lower or 'conference' in full_text_lower or 'proceeding' in full_text_lower or 'konferans' in full_text_lower:
                        pub_type = 'Conference'
                    elif 'kitap' in full_text_lower or 'chapter' in full_text_lower:
                        pub_type = 'Book'

                    # Normalization function
                    def normalize_title(t):
                         return re.sub(r'[^a-zA-Z0-9]', '', t.lower())

                    # Dedup Check
                    normalized = normalize_title(final_title)
                    
                    # AÅŸÄ±rÄ± kÄ±sa veya anlamsÄ±zlarÄ± ele
                    if len(normalized) < 15: continue
                    
                    if normalized not in seen_normalized:
                        seen_normalized.add(normalized)
                        # Store as dict
                        fetched_items.append({
                            'title': final_title,
                            'type': pub_type,
                            'year': found_year,
                            'raw_text': full_text[:200]
                        })
                
                data[key] = fetched_items
                print(f"   -> {tab_text}: {len(data[key])} Ã¶ge (Dedup sonrasÄ±)")
                
                # Ä°statistik: TÃ¼r daÄŸÄ±lÄ±mÄ±
                if key == 'publications':
                    type_counts = {}
                    for item in fetched_items:
                        t = item['type']
                        type_counts[t] = type_counts.get(t, 0) + 1
                    print(f"      ğŸ“Š TÃ¼r DaÄŸÄ±lÄ±mÄ±: {type_counts}")

            except Exception as e:
                print(f"   âš ï¸ Sekme HatasÄ± ({tab_text}): {e}")
                pass
        
        return data

        return data

    except Exception as e:
        print(f"âŒ YÃ–K Genel Tarama HatasÄ±: {e}")
        return data
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass # Zaten kapalÄ±ysa sorun yok

# Eski fonksiyonlarÄ±n yerine wrapperlar (uyumluluk iÃ§in)
def get_yok_publications(yok_id, name=None):
    return scrape_yok_profile(yok_id, name)['publications']

def get_yok_projects(yok_id, name=None):
    return scrape_yok_profile(yok_id, name)['projects']

def get_yok_paper_count(yok_id, name=None):
    # EÄŸer isim verilmiÅŸse scrape fonksiyonu akÄ±llÄ± arama yapar
    data = scrape_yok_profile(yok_id, name)
    pubs = data['publications']
    
    # EÄŸer yeni bir ID Ã§Ã¶zÃ¼ldÃ¼yse, onu da dÃ¶ndÃ¼rmenin bir yolunu bulmalÄ±yÄ±z.
    # Ancak bu basit wrapper sadece sayÄ± dÃ¶ndÃ¼rÃ¼yor.
    # ID gÃ¼ncelleme iÅŸini Ã§aÄŸÄ±ran yer (app.py) yapmalÄ±.
    
    return len(pubs), data.get('resolved_id')
